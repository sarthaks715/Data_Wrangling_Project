{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Sarthak Sareen\n",
    "#### Student ID: 30761182\n",
    "\n",
    "Date: 13/9/2020\n",
    "\n",
    "\n",
    "\n",
    "* pandas (for dataframe, included in Anaconda Python 2.7)  \n",
    "* langid (to check string language, included in Anaconda Python 2.7) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task we are analyzing textual data, i.e., converting the extracted data into a proper format. we will preprocess a set\n",
    "\n",
    "of tweets and convert them into numerical representations (which are suitable for input into recommender-systems/ information-\n",
    "\n",
    "retrieval algorithms). The data-set that we provide contains 80+ days of COVID-19 related tweets (from late March to\n",
    "\n",
    "mid July 2020). We have  extract and transform the information of the excel file performing the following task:\n",
    "\n",
    "1) making a vocab .txt \n",
    "\n",
    "2) For each day calculate the top 100 frequent unigram and top-100 frequent bigrams and saving it into a file 100bi.txt and \n",
    "   \n",
    "   100.uni.txt\n",
    "\n",
    "3) Generate the sparse representation (i.e., doc-term matrix) of the excel file according to\n",
    "\n",
    "   the structure of the sample_countVec.txt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the librarbies for the preprocess of the text.\n",
    "from __future__ import division\n",
    "import langid\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file \n",
    "excel = pd.ExcelFile('30761182.xlsx')\n",
    "#making empty dictonary\n",
    "di ={}\n",
    "#looping in the range of the length of the excel sheets\n",
    "for i in range(len(excel.sheet_names)): \n",
    "    #making a datframe of the first excel sheet\n",
    "    df = excel.parse(i)\n",
    "    #dropping the NA's from the dataframe\n",
    "    df = df.dropna(how='all',axis=1)\n",
    "    df = df.dropna(how='all',axis=0)\n",
    "    #check the 1st column is text or not\n",
    "    if df.columns[0]!= 'text':\n",
    "        ##renaming the column from row 1\n",
    "        df.rename(columns=df.iloc[0], inplace = True)\n",
    "        #dropping the 1 st row\n",
    "        df.drop(df.index[0], inplace = True)\n",
    "    #looping in the dataframe text column values\n",
    "    for y in df.text.values:\n",
    "        try:\n",
    "        #checking the text of the dataframe text column is english \n",
    "            if langid.classify(y)[0]=='en':\n",
    "                #check if key in the dictonary or not\n",
    "                if excel.sheet_names[i] in di:\n",
    "                    #appending the values \n",
    "                    di[excel.sheet_names[i]] = di[excel.sheet_names[i]] + '\\n' + y.lower()\n",
    "                else:\n",
    "                    #making new keys with values\n",
    "                     di[excel.sheet_names[i]] = y.lower()\n",
    "        # text of the dataframe text column is not english\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) In the above cell the excel file is being loaded with 80+ sheets which is being saved in the excel named variable.\n",
    "\n",
    "2) Making a new empty dictonary di.\n",
    "\n",
    "3) Looping till the range of the length of the excel.sheet_names i.e 0,1,2...\n",
    "\n",
    "    i) making a dataframe of the excel sheet number \n",
    "    \n",
    "    ii) dropping the NA's from all the columns\n",
    "    \n",
    "    iii) dropping the Na's from all the rows\n",
    "    \n",
    "    iv) checking if the dataframe columns is not  \"text\" then\n",
    "      \n",
    "        --rename the columns of the datframe with the first row of the datframe and making it inplace to true that is saving             the dataframe changes.\n",
    "        \n",
    "        --then dropping the first row as it will duplicate as we have made the first row as columns.\n",
    "        \n",
    "        -- looping in the dataframe text column values \n",
    "            \n",
    "           *trying if the text column value is in english \n",
    "              \n",
    "              A) checking if the key is present in the dictonary or not \n",
    "                   \n",
    "                   -- adding the elements in the same key and lowering the case of the text also\n",
    "                   \n",
    "              B) if key is not present then making a new key as the excel sheet date as key and value of its text column\n",
    "            \n",
    "            * except is the text is not en using langid library then pass. It will not add that row into the dictonary.\n",
    "      \n",
    "4) getting the desired dictionary with keys as dates of the excel sheets and values as the data of teh text column.\n",
    "               \n",
    "              \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new dictonary \n",
    "new_dict ={}\n",
    "#initialize the tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "#making tokens and making a new dictonary\n",
    "for i,k in di.items():\n",
    "    new_dict[i] = tokenizer.tokenize(di[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we are creating tokens of the dictionary from the regex we have given and making a new dictionary with the keys as the date and values as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the stopwords and saving it in a list called stopped\n",
    "with open('stopwords_en.txt') as infile:\n",
    "    stopped = infile.readlines() \n",
    "\n",
    "#stripping the list \n",
    "stopped = set([line.rstrip('\\n') for line in stopped])\n",
    "\n",
    "#making an empty dictionary\n",
    "uni_dict_new ={}\n",
    "\n",
    "#looping the new_dict and saving into new dic where the values are not present in stopwords list and the length >=3\n",
    "for i,j in new_dict.items():\n",
    "    uni_dict_new[i] =[w for w in j if w not in stopped and len(w) >= 3]\n",
    "                 \n",
    "#making bag of bag of tokens\n",
    "freq_list = FreqDist(list(chain.from_iterable([set(value) for value in uni_dict_new.values()])))\n",
    "\n",
    "#making new dictionary \n",
    "new_freq_dic ={}\n",
    "\n",
    "#looping in the freq_list dic if the value is greater tha 60 and smaller than 5 then pass else new dictonary add\n",
    "for i,j in freq_list.items():\n",
    "    if j > 60 or j < 5 :\n",
    "        pass\n",
    "    else:\n",
    "        new_freq_dic[i] = j\n",
    "#making new dict empty stemmed \n",
    "uni_dict_stem ={}\n",
    "\n",
    "#making new dict empty stemmed and freqdist\n",
    "uni_dict_stem_final ={}\n",
    "\n",
    "#looping in the dic where stopped words and length >= are removed from the dic\n",
    "for i,j in uni_dict_new.items():\n",
    "    #stemming the values\n",
    "    uni_dict_stem[i] = [stem.stem(x) for x in j if x in new_freq_dic.keys()]\n",
    "\n",
    " #looping in the dic where the values are stemmed\n",
    "for i,j in uni_dict_stem.items():\n",
    "    #using freq dist to calculate count\n",
    "    uni_dict_stem_final[i] = FreqDist(j).most_common(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Above cell :\n",
    "\n",
    "1) Reading the stopped words file\n",
    "\n",
    "2) splitting the list to make list with only stop words\n",
    "\n",
    "3) Looping in the dictornary of the dat athat we have created and adding the key as teh date and teh value which are not in        \n",
    "   stopwords list and the length of the value is greater than 3.\n",
    "   \n",
    "4) Making freq_list that is the dictonary with words and the count of that word using the function freq dist. the arugument of    \n",
    "   the freq dist I have given as the values of the dict i.e dictinory removed from stopwords and the len of the values of the      \n",
    "   dictionary is greater than 3.\n",
    "\n",
    "5) now removing the words which have count > 60 and <5 and making a new dict named new_freq_dict which has keys as words and     \n",
    "   value as the count of that word.\n",
    "   \n",
    "6) Now creating two dictonary with the steemed words of the values of the dictionary i.e unit_dict_stem and as dictonary with      \n",
    "   stem words and count of that words using Freqdist i.e uni_dict_stem_final.\n",
    "   \n",
    "7) We get a dictonary which have date as the key and values which habve the words and teh count of that word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilizing the string\n",
    "string_uni =''\n",
    "#looping in final dic of the uni grams\n",
    "for i,j in uni_dict_stem_final.items():\n",
    "    string_uni = string_uni + str(i) + \":\" + str(j)\n",
    "    #string_uni = string_uni + str(x) \n",
    "    string_uni = string_uni + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell making a string which is according to the output using the dictonary of unigram that I have made above i.e uni_dict_stem_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wring the string into the file\n",
    "with open(\"30761182_100uni.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(string_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting the string into the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##bigrams\n",
    "bgm = nltk.collocations.BigramAssocMeasures()\n",
    "#new dictonary for bigrams empty\n",
    "bi_dict = {}\n",
    "#looping in the new_dict\n",
    "for i,j in new_dict.items():\n",
    "    #passing the values of the dictionary to the bigram collaction finder\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(j)\n",
    "    #taing 200 bigrams\n",
    "    freq = finder.nbest(bgm.pmi,200)\n",
    "    lst = (sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:100])\n",
    "    #making a new dict of the bigrams\n",
    "    bi_dict[i] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell:\n",
    "\n",
    "1) making the object of nltk collactions bigram measures \n",
    "\n",
    "2) making new bi_dict emoty for the bigrams\n",
    "\n",
    "3) looping into the main dictonary which have key as the date and the tokens as the values\n",
    "   \n",
    "   --passing the value of the key into the nltk collocations bigram collaction finder \n",
    "   \n",
    "   -- taking top 200 bigrams\n",
    "   \n",
    "   --making a new dictionary with key as the date and key include the bigram adn the count of that bigram\n",
    "\n",
    "4) The dictonary which is bi_dict is the desired dictinoary which is used to write int the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty string\n",
    "string_bi =''\n",
    "#string concate looping in the bi_dict\n",
    "for i,j in bi_dict.items():\n",
    "    string_bi = string_bi + str(i) + \":\" + str(j) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell making a string which is according to the output using the dictonary of bigram that I have made above i.e bi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"30761182_100bi.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(string_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting the string into the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VOCAB\n",
    "#empty list of teh vocab\n",
    "bi_list_vocab1 = []\n",
    "\n",
    "#making bag of bag from the values of the dictonary \n",
    "words = list(chain.from_iterable(new_dict.values()))\n",
    "\n",
    "#passing the bag of bag to the bigram collacations\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "\n",
    "#taking top 200 bigrams\n",
    "freq = finder.nbest(bgm.pmi,200)\n",
    "\n",
    "#empty string\n",
    "s=''\n",
    "#looping in the top 200 bigrams\n",
    "for i in freq:\n",
    "    #adding in the string with \"_\"\n",
    "    s= s + i[0] + \"_\" + i[1] + \"\\n\"\n",
    "\n",
    "#splitting the string \n",
    "list_final = s.split('\\n')\n",
    "list_final = list_final[:-1]\n",
    "\n",
    "#looping in the unigram list which is stemmed \n",
    "for i,j in uni_dict_stem.items():\n",
    "    for x in j:\n",
    "        #appending bigrams in the list\n",
    "        list_final.append(x)\n",
    "#making set of the list of vocab\n",
    "last = set(list_final)\n",
    "#sorting the list\n",
    "sorted_list_final = sorted(last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell:\n",
    "\n",
    "1) making a list of vocab empty\n",
    "\n",
    "2) making a bag og bags of the words with the arugument of teh values of the dictinary in which there are all the tokens of the \n",
    "\n",
    "   text without removing anything.\n",
    "   \n",
    "3) taking the top 200 bigrams to freq\n",
    "\n",
    "4) Now looping into the freq and adding \"_ into the both the words of the bigrams because we have to represent the bigram like   \n",
    "   this.\n",
    "   \n",
    "5) spliting the string and making it into list\n",
    "\n",
    "6) slicing the list to remove \"\\n\".\n",
    "\n",
    "7) Now lopping into the dictonary of unigram where the values are stemmed that is being vreaed where we have created unigrams \n",
    "   \n",
    "   and appending it into the list of bigrams.\n",
    "   \n",
    "8) Now making the list a set to remove the repeated words\n",
    "\n",
    "9) sorting the list \n",
    "\n",
    "10) the list is the list of vocabs.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vocab = 0\n",
    "#initilise empty string\n",
    "string_vocab=''\n",
    "#looping in the list of vocabs\n",
    "for i in sorted_list_final:\n",
    "    #string concate \n",
    "    string_vocab = string_vocab + str(i) + \":\" + str(count_vocab)+ \"\\n\"\n",
    "    #index of the word\n",
    "    count_vocab=count_vocab+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell:\n",
    "\n",
    "iterating into the list of the vocabs and concating the string with the index of the number which is being calculated by the variable count_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"30761182_vocab.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(string_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting the string into the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##counter vector\n",
    "#making emoty counter_vector dic\n",
    "counter_bi_dict = {}\n",
    "\n",
    "#looping in the original dic where nothing is being removed\n",
    "for i,j in new_dict.items():\n",
    "    #giving the values of the list to bigram collactions \n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(j)\n",
    "    frequency = bgm.pmi\n",
    "    lst = (sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0])))\n",
    "    #making new dict with key as date and values as bigrams\n",
    "    counter_bi_dict[i] = lst\n",
    "\n",
    "#new empty final_counter_bi_dic\n",
    "final_counter_bi_dic ={}\n",
    "\n",
    "#looping in the freq which contains bigrams which are in the vocabs \n",
    "for i in freq:\n",
    "    #looping in the above created dic \n",
    "    for k,l in counter_bi_dict.items():\n",
    "        #iterating in the values of the dict\n",
    "        for x in l:\n",
    "            #check bigram in freq == counter_bi_dict\n",
    "            if i == x[0]:\n",
    "                #check if key present\n",
    "                if k in final_counter_bi_dic:\n",
    "                    #adding into the new final dict append in the present key\n",
    "                    final_counter_bi_dic[k].append(i)\n",
    "                else:\n",
    "                    #adding into the new key and values \n",
    "                    final_counter_bi_dic[k] = [i]\n",
    "\n",
    "#adding the bigrams in the uni_dict_stem dictionary                    \n",
    "for i,j in final_counter_bi_dic.items():\n",
    "    for l,m in uni_dict_stem.items():\n",
    "        if i==l:\n",
    "            for x in j:\n",
    "                uni_dict_stem[l].append(str(x[0]) + \"_\" + str(x[1]))                   \n",
    "    \n",
    "#making counter vector    \n",
    "vocab_dict = {}\n",
    "vector_dict ={}\n",
    "last_dict={}\n",
    "#making dict of vocab having key as words and value as index\n",
    "i=0\n",
    "for w in sorted_list_final:\n",
    "    vocab_dict[w] = i\n",
    "    i = i+1\n",
    "    \n",
    "stem_dict = {}   \n",
    "string_counter_final=''\n",
    "\n",
    "#looping in uni_dict_stem\n",
    "for x,y in uni_dict_stem.items():\n",
    "    #occurence of the tokens id\n",
    "    last_val = [vocab_dict[k] for k in y]\n",
    "    #string concate\n",
    "    string_counter_final = string_counter_final + str(x) + \",\"\n",
    "    #calculating the occurence of token\n",
    "    for l,m in FreqDist(last_val).items():\n",
    "        #string concate for output\n",
    "        string_counter_final = string_counter_final + str(l) + \":\" + str(m) + \",\"\n",
    "    string_counter_final = string_counter_final[:-1]\n",
    "    string_counter_final = string_counter_final +'\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell:\n",
    "\n",
    "1) First we have looped in the new_dict dictionary which has the date as the key and tokens as teh values of the keys without      \n",
    "   any removal of teh stopwords or anything.\n",
    "\n",
    "2) Then the values of the above dictionary is being pasted to the nltk.collocations.BigramCollocationFinder to find the bigrams    \n",
    "   in the tokens\n",
    "\n",
    "3) putting it into a new dict keys as the date and the values as the bigrams.\n",
    "\n",
    "4) loopinmg in the freq which contains the valid bigrams and another loop inside it in the dictionary which contains the          \n",
    "   bigrams created in 2nd step.\n",
    "   \n",
    "   checking if the bigram in the freq is equal to the bigram in the dictionary then :\n",
    "      --check key is present in the new dict if satisfy then append in the existing key into values\n",
    "      -- if not then make new key and add its values to the key\n",
    "      \n",
    "5) Looping into the final_counter_bi_dic and inside loop in uni_dict_stem if the keys are same then the bigrams get appended to    \n",
    "   the uni_dict_stem.\n",
    "   \n",
    "   \n",
    "6) making dict of vocab having key as words and value as index\n",
    "\n",
    "7) looping into the uni_dict_stem calculatimg the occurence of each id of the vocab in last_vocab. Then inside loop in the \n",
    "\n",
    "   last_val calculating the Freq dist of the keys of the values of the dict and concating into the string of the counter vector \n",
    "   \n",
    "   output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"30761182_countVec.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(string_counter_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting the string into the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all teh excel files sheet and add the sheet sinto dataframes to make a dictonary of the text column in the sheet.\n",
    "\n",
    "For Unigrams.txt:\n",
    "\n",
    "Read stopwords file and then removed the stopped words and teh words which are less than 3 length from the dictonary. Then removed the rare tokens and the context dependent i.e the words that comes <5 or greater than 60 times in the documents. Then stemmed the values of the dictinary values and then count the occurence of the words using FreqDIst. The dictonary at the end the desired dict which have dsate as a key and values as the unigrams.\n",
    "\n",
    "\n",
    "For Bigrams.txt\n",
    "\n",
    "make an object of the nltk.collocations.BigramAssocMeasures(). Then make a dictonary with the topp 200 bigrams using the finder.nbest function and then adding them into a dictionary. \n",
    "\n",
    "\n",
    "For countvector.txt\n",
    "\n",
    "Making bigrams from the new_dict which have all the bigrams of the text according to the date. Then checking the bigrams created from the valid bigrams that created in the bigram.txt and making a new dict. Then appending the unigrams to the dictionary from the stemmed dict that created in the creating unigram.txt steps. Then using Freq dist calculated the occurence of each value and desired output file.\n",
    "\n",
    "For vocab.txt\n",
    "used the unigrams from the above steps while creating unigram.txt then created bigrams. Then add both the unigrams and bigrams into one list and add then into a list. Made a set of that list and sort them. Then concated into the string and write the string into the file.\n",
    "\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
